"""
RAG Index Builder - LanceDB Implementation.

Builds vector index from Agent OS standards using LanceDB for semantic search.
Switched from ChromaDB to LanceDB for:
- Better metadata filtering (WHERE clauses at DB level)
- Cleaner hot reload (no singleton client conflicts)
- Incremental updates (update vs full rebuild)

100% AI-authored via human orchestration.
"""

import argparse
import json
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, cast

import lancedb

# Add mcp_server to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))
from mcp_server.chunker import AgentOSChunker

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


class IndexBuilder:
    """Builds and maintains LanceDB vector index for Agent OS standards."""

    def __init__(
        self,
        index_path: Path,
        standards_path: Path,
        usage_path: Optional[Path] = None,
        workflows_path: Optional[Path] = None,
        embedding_provider: str = "local",
        embedding_model: str = "all-MiniLM-L6-v2",
    ):
        """
        Initialize index builder.

        Args:
            index_path: Path to store vector index (LanceDB directory)
            standards_path: Path to Agent OS standards directory
            usage_path: Path to Agent OS usage documentation directory (optional)
            workflows_path: Path to workflows directory with metadata.json files (optional but recommended)
            embedding_provider: Provider for embeddings ("local" default/free or "openai" costs money)
            embedding_model: Model to use (all-MiniLM-L6-v2 for local, text-embedding-3-small for openai)
        """
        self.index_path = index_path
        self.standards_path = standards_path
        self.usage_path = usage_path
        self.workflows_path = workflows_path

        # Build list of source paths to scan
        self.source_paths = [standards_path]
        if usage_path and usage_path.exists():
            self.source_paths.append(usage_path)
            logger.info(f"Including usage docs from: {usage_path}")
        if workflows_path and workflows_path.exists():
            self.source_paths.append(workflows_path)
            logger.info(f"Including workflow metadata from: {workflows_path}")

        self.embedding_provider = embedding_provider
        self.embedding_model = embedding_model

        # Initialize embedding model
        self.local_model: Any = None
        if self.embedding_provider == "local":
            logger.info(
                "Using local embeddings (sentence-transformers) - FREE & OFFLINE"
            )
            from sentence_transformers import SentenceTransformer

            self.local_model = SentenceTransformer(embedding_model)
        else:
            self.local_model = None

        # Ensure paths exist
        self.index_path.mkdir(parents=True, exist_ok=True)

        # Initialize LanceDB connection
        logger.info(f"Initializing LanceDB at {index_path}")
        self.db = lancedb.connect(str(index_path))
        logger.info("LanceDB initialized successfully")

    def generate_embedding(self, text: str) -> List[float]:
        """
        Generate vector embedding for text.

        Args:
            text: Text to embed

        Returns:
            Embedding vector (384 dimensions for local, 1536 for openai)

        Raises:
            Exception: If embedding generation fails
        """
        if self.embedding_provider == "local":
            try:
                # Local embeddings using sentence-transformers
                # Returns 384-dimensional vector
                embedding = self.local_model.encode(text, convert_to_numpy=True)
                return cast(List[float], embedding.tolist())
            except Exception as e:
                logger.error(f"Failed to generate local embedding: {e}")
                raise
        elif self.embedding_provider == "openai":
            try:
                import openai

                # OpenAI embeddings (requires API key, costs money)
                response = openai.embeddings.create(
                    model=self.embedding_model, input=text
                )
                # OpenAI SDK returns embedding as list[float] but type-stubbed as Any
                return response.data[0].embedding  # type: ignore[no-any-return]
            except Exception as e:
                logger.error(f"Failed to generate OpenAI embedding: {e}")
                raise
        else:
            raise ValueError(f"Unknown embedding provider: {self.embedding_provider}")

    def build_index(
        self, force: bool = False, incremental: bool = True
    ) -> Dict[str, Any]:
        """
        Build vector index from Agent OS files.

        Steps:
        1. Check if rebuild needed (unless force=True)
        2. Determine build strategy (full vs incremental)
        3. Find files to process
        4. Chunk files and generate embeddings
        5. Update/create LanceDB table
        6. Save metadata

        Args:
            force: Force full rebuild even if index is fresh
            incremental: Use incremental updates (only process changed files)

        Returns:
            Build statistics dictionary

        Raises:
            Exception: If build fails
        """
        logger.info("Starting RAG index build")
        start_time = datetime.now()

        # Check if rebuild needed
        if not force and self._is_index_fresh():
            logger.info("Index is fresh, skipping rebuild (use --force to rebuild)")
            return {"status": "skipped", "reason": "index_fresh"}

        # Determine build strategy
        table_exists = "agent_os_standards" in self.db.table_names()
        use_incremental = incremental and table_exists and not force

        if use_incremental:
            logger.info("üìù Using incremental update (only processing changed files)")
            changed_files = self._get_changed_files()

            if not changed_files:
                logger.info("No files changed, index is up to date")
                return {"status": "skipped", "reason": "no_changes"}

            logger.info(f"Found {len(changed_files)} changed files")
            files_to_process = changed_files

            # Open existing table
            table = self.db.open_table("agent_os_standards")

            # Delete old chunks for changed files
            logger.info("üóëÔ∏è  Removing old chunks for changed files...")
            for md_file in changed_files:
                # Calculate relative path from correct base
                rel_path = str(md_file.relative_to(self.standards_path.parent))
                try:
                    table.delete(f"file_path = '{rel_path}'")
                    logger.debug(f"Deleted chunks for {rel_path}")
                except Exception as e:
                    logger.warning(f"Failed to delete chunks for {rel_path}: {e}")
        else:
            if force:
                logger.info("üîÑ Force rebuild requested - processing all files")
            else:
                logger.info("üîÑ Initial build - processing all files")
            # Scan all source directories
            files_to_process = []
            for source_path in self.source_paths:
                files_to_process.extend(list(source_path.rglob("*.md")))

        # Initialize chunker
        chunker = AgentOSChunker()

        # Chunk files (all or changed)
        logger.info(f"Processing {len(files_to_process)} markdown files")
        all_chunks = []
        for idx, filepath in enumerate(files_to_process, 1):
            try:
                logger.info(f"[{idx}/{len(files_to_process)}] Chunking {filepath.name}")
                chunks = chunker.chunk_file(filepath)
                all_chunks.extend(chunks)
                logger.debug(f"  Generated {len(chunks)} chunks")
            except Exception as e:
                logger.error(f"Failed to chunk {filepath}: {e}")
                # Continue processing other files
                continue

        logger.info(
            f"Generated {len(all_chunks)} chunks from {len(files_to_process)} files"
        )

        # Convert chunks to LanceDB format with embeddings
        logger.info("Generating embeddings for all chunks...")
        records = []
        for idx, chunk in enumerate(all_chunks, 1):
            if idx % 100 == 0:
                logger.info(f"  Embedding chunk {idx}/{len(all_chunks)}")

            try:
                # Generate embedding
                embedding = self.generate_embedding(chunk.content)

                # Convert absolute file path to relative path
                abs_path = Path(chunk.file_path)
                rel_file_path = str(abs_path.relative_to(self.standards_path.parent))

                # Create record with embedding and metadata
                record = {
                    "chunk_id": chunk.chunk_id,
                    "content": chunk.content,
                    "vector": embedding,
                    "file_path": rel_file_path,
                    "section_header": chunk.section_header,
                    "parent_headers": json.dumps(chunk.metadata.parent_headers),
                    "token_count": chunk.tokens,
                    "phase": chunk.metadata.phase if chunk.metadata.phase else 0,
                    "framework_type": chunk.metadata.framework_type or "",
                    "category": chunk.metadata.category or "",
                    "is_critical": chunk.metadata.is_critical,
                    "tags": json.dumps(chunk.metadata.tags),
                }
                records.append(record)

            except Exception as e:
                logger.error(f"Failed to embed chunk {idx}: {e}")
                continue

        logger.info(f"Successfully created {len(records)} records with embeddings")

        # Update or create table
        if use_incremental:
            # Add new records to existing table
            logger.info(
                f"‚ûï Adding {len(records)} new/updated records to existing table..."
            )
            table.add(records)
            total_chunks = table.count_rows()
            logger.info(f"‚úÖ Table updated - now contains {total_chunks} total records")
        else:
            # Full rebuild - drop and recreate table
            try:
                if table_exists:
                    logger.info("Dropping existing table for full rebuild")
                    self.db.drop_table("agent_os_standards")
            except Exception as e:
                logger.warning(f"Could not drop existing table: {e}")

            logger.info(f"Creating new table with {len(records)} records...")
            table = self.db.create_table("agent_os_standards", records)
            total_chunks = len(records)
            logger.info(f"‚úÖ Table created with {total_chunks} records")

        # Save metadata with file modification times
        build_time = datetime.now()
        elapsed = (build_time - start_time).total_seconds()

        # Collect all file mtimes for change detection
        file_mtimes = {}
        for source_path in self.source_paths:
            for md_file in source_path.rglob("*.md"):
                rel_path = str(md_file.relative_to(self.standards_path.parent))
                file_mtimes[rel_path] = md_file.stat().st_mtime

        metadata = {
            "build_time": build_time.isoformat(),
            "source_files": len(file_mtimes),
            "total_chunks": total_chunks,
            "embedding_provider": self.embedding_provider,
            "embedding_model": self.embedding_model,
            "build_duration_seconds": elapsed,
            "build_type": "incremental" if use_incremental else "full",
            "files_mtimes": file_mtimes,
        }

        metadata_file = self.index_path / "metadata.json"
        metadata_file.write_text(json.dumps(metadata, indent=2))
        logger.info(f"Metadata saved to {metadata_file}")

        build_type = "incremental update" if use_incremental else "full build"
        logger.info(f"‚úÖ Index {build_type} complete in {elapsed:.1f}s")
        return {
            "status": "success",
            "chunks": total_chunks,
            "files": len(file_mtimes),
            "files_processed": len(files_to_process),
            "duration_seconds": elapsed,
            "build_type": "incremental" if use_incremental else "full",
        }

    def _get_changed_files(self) -> List[Path]:
        """
        Get list of files that changed since last build.

        Returns:
            List of file paths that need reprocessing
        """
        metadata_file = self.index_path / "metadata.json"

        # No metadata = all files are "changed"
        if not metadata_file.exists():
            all_files = []
            for source_path in self.source_paths:
                all_files.extend(list(source_path.rglob("*.md")))
            return all_files

        try:
            metadata = json.loads(metadata_file.read_text())
            file_mtimes = metadata.get("files_mtimes", {})

            changed_files = []
            current_files = set()

            for source_path in self.source_paths:
                for md_file in source_path.rglob("*.md"):
                    rel_path = str(md_file.relative_to(self.standards_path.parent))
                    current_files.add(rel_path)
                    current_mtime = md_file.stat().st_mtime

                    # File is new or modified
                    if (
                        rel_path not in file_mtimes
                        or file_mtimes[rel_path] != current_mtime
                    ):
                        changed_files.append(md_file)

            # Detect and handle deleted files (in old metadata but not in current files)
            deleted_files = set(file_mtimes.keys()) - current_files
            if deleted_files:
                logger.info(f"Detected {len(deleted_files)} deleted files")
                # Delete chunks for deleted files from the index
                if "agent_os_standards" in self.db.table_names():
                    try:
                        table = self.db.open_table("agent_os_standards")
                        for deleted_path in deleted_files:
                            try:
                                table.delete(f"file_path = '{deleted_path}'")
                                logger.info(
                                    f"üóëÔ∏è  Removed chunks for deleted file: {deleted_path}"
                                )
                            except Exception as e:
                                logger.warning(
                                    f"Failed to delete chunks for {deleted_path}: {e}"
                                )
                    except Exception as e:
                        logger.warning(f"Failed to open table for deletion: {e}")

            return changed_files

        except Exception as e:
            logger.warning(f"Error detecting changed files: {e}")
            # Fall back to full rebuild
            all_files = []
            for source_path in self.source_paths:
                all_files.extend(list(source_path.rglob("*.md")))
            return all_files

    def _is_index_fresh(self) -> bool:
        """
        Check if index needs rebuild.

        Returns:
            True if index is fresh and doesn't need rebuild
        """
        metadata_file = self.index_path / "metadata.json"

        # No metadata = needs build
        if not metadata_file.exists():
            return False

        # Check if table exists
        if "agent_os_standards" not in self.db.table_names():
            return False

        try:
            # Read metadata
            metadata = json.loads(metadata_file.read_text())
            build_time = datetime.fromisoformat(metadata["build_time"])

            # Check if any source files are newer than build
            for source_path in self.source_paths:
                for md_file in source_path.rglob("*.md"):
                    file_mtime = datetime.fromtimestamp(md_file.stat().st_mtime)
                    if file_mtime > build_time:
                        logger.debug(f"File {md_file.name} modified after build")
                        return False

            return True

        except Exception as e:
            logger.warning(f"Could not check index freshness: {e}")
            return False


def main():
    """CLI entry point for index builder."""
    parser = argparse.ArgumentParser(
        description="Build RAG vector index from Agent OS standards"
    )

    parser.add_argument(
        "--force",
        action="store_true",
        help="Force full rebuild even if index exists and is fresh",
    )
    parser.add_argument(
        "--no-incremental",
        action="store_true",
        help="Disable incremental updates (force full rebuild)",
    )

    parser.add_argument(
        "--provider",
        default="local",
        choices=["local", "openai"],
        help="Embedding provider: 'local' (FREE, offline, default) or 'openai' (~3-5%% better, costs money)",
    )

    parser.add_argument(
        "--model",
        default=None,
        help="Embedding model (defaults: all-MiniLM-L6-v2 for local, text-embedding-3-small for openai)",
    )

    parser.add_argument(
        "--index-path",
        type=Path,
        default=None,
        help="Path to store vector index (default: .praxis-os/.cache/vector_index)",
    )

    parser.add_argument(
        "--standards-path",
        type=Path,
        default=None,
        help="Path to Agent OS standards (default: .praxis-os/standards)",
    )

    parser.add_argument(
        "--usage-path",
        type=Path,
        default=None,
        help="Path to Agent OS usage docs (default: auto-detect from standards-path)",
    )

    parser.add_argument(
        "--workflows-path",
        type=Path,
        default=None,
        help="Path to Agent OS workflows (default: auto-detect from standards-path)",
    )

    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Enable verbose logging",
    )

    args = parser.parse_args()

    # Configure logging level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # Determine paths
    script_dir = Path(__file__).parent
    repo_root = script_dir.parent

    # Default paths depend on whether we're in source repo or installed location
    # If script is at .praxis-os/scripts/, then repo_root is .praxis-os/
    # If script is at agent-os-enhanced/scripts/, then repo_root is agent-os-enhanced/

    # Detect if we're in an installed location
    is_installed = (repo_root / "standards").exists() and not (
        repo_root / "universal"
    ).exists()

    if is_installed:
        # Installed in .praxis-os/ directory
        index_path = args.index_path or (repo_root / ".cache" / "vector_index")
        standards_path = args.standards_path or (repo_root / "standards")
        usage_path = args.usage_path or (repo_root / "usage")
        workflows_path = args.workflows_path or (repo_root / "workflows")
    else:
        # Running from source repository
        source_root = repo_root.parent  # go up one more level to project root
        index_path = args.index_path or (
            source_root / ".praxis-os" / ".cache" / "vector_index"
        )
        standards_path = args.standards_path or (repo_root / "universal" / "standards")
        usage_path = args.usage_path or (repo_root / "universal" / "usage")
        workflows_path = args.workflows_path or (repo_root / "universal" / "workflows")

    # Validate standards path exists
    if not standards_path.exists():
        logger.error(f"Standards path does not exist: {standards_path}")
        logger.info("Please specify --standards-path or run from repository root")
        sys.exit(1)

    # Check OpenAI API key if using OpenAI
    if args.provider == "openai":
        import os

        if not os.getenv("OPENAI_API_KEY"):
            logger.error("OPENAI_API_KEY environment variable not set")
            logger.info("Switching to local embeddings (free, offline)")
            args.provider = "local"

    try:
        # Determine model based on provider if not specified
        if args.model is None:
            if args.provider == "local":
                args.model = "all-MiniLM-L6-v2"
            else:
                args.model = "text-embedding-3-small"

        # Build index
        builder = IndexBuilder(
            index_path=index_path,
            standards_path=standards_path,
            usage_path=usage_path,
            workflows_path=workflows_path,  # NEW: Include workflows
            embedding_provider=args.provider,
            embedding_model=args.model,
        )

        result = builder.build_index(
            force=args.force,
            incremental=not args.no_incremental,
        )

        if result["status"] == "success":
            logger.info("‚úÖ Index built successfully!")
            sys.exit(0)
        elif result["status"] == "skipped":
            logger.info("‚úÖ Index is already fresh!")
            sys.exit(0)
        else:
            logger.error(f"‚ùå Index build failed: {result}")
            sys.exit(1)

    except KeyboardInterrupt:
        logger.warning("Build interrupted by user")
        sys.exit(130)
    except Exception as e:
        logger.error(f"Fatal error during index build: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
