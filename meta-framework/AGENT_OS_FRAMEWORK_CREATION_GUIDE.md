# Agent OS Framework Creation Guide

**Version**: 1.0  
**Purpose**: Portable methodology for creating deterministic, high-quality AI-assisted workflows  
**Transferability**: Apply to any repository or domain requiring systematic AI execution  
**Source**: Distilled from python-sdk Agent OS methodologies and V3 framework success patterns

---

## ğŸ¯ **What Is This?**

A **meta-framework** for creating frameworks - a systematic approach to designing AI-assisted workflows that achieve:
- **85-95% execution consistency** (vs 60-70% without frameworks)
- **2-3x better context efficiency** (15-25% vs 50-75% utilization)
- **Automated quality enforcement** (100% validation coverage)
- **Scalable complexity management** (linear growth, not exponential)

**Use Cases**:
- Schema extraction frameworks
- Code generation workflows
- Documentation creation systems
- Testing automation frameworks
- Migration/refactoring processes
- Research and analysis workflows

---

## ğŸ“Š **Core Engineering Principles**

### **Principle 1: LLM Constraint Awareness**

**Context Window Optimization**:
| Context Use | File Size | Attention Quality | AI Success Rate |
|-------------|-----------|-------------------|-----------------|
| Optimal | â‰¤100 lines | 95%+ | 85%+ |
| Degraded | 200-500 lines | 70-85% | 60-75% |
| Failure | >500 lines | <70% | <50% |

**Key Insight**: Small, focused files compensate for LLM attention limitations.

---

### **Principle 2: Three-Tier Architecture**

**Tier 1: Side-Loaded Context** (AI reads during execution)
- **Size**: â‰¤100 lines per file
- **Purpose**: Execution instructions with binding commands
- **Pattern**: Single-responsibility, phase-specific guidance
- **Examples**: `phase-1-analysis.md`, `task-2-validation.md`

**Tier 2: Active Read Context** (AI reads on-demand)
- **Size**: 200-500 lines per file
- **Purpose**: Comprehensive methodology and architecture
- **Pattern**: Foundation documents, complete specifications
- **Examples**: `README.md`, `METHODOLOGY.md`, `framework-core.md`

**Tier 3: Output Artifacts** (AI generates, never re-consumes)
- **Size**: Unlimited
- **Purpose**: Generated deliverables
- **Pattern**: Code, schemas, documentation, reports
- **Examples**: Generated test files, extracted schemas, analysis reports

**Critical**: AI must NEVER re-read Tier 3 outputs (context pollution).

---

### **Principle 3: Command Language Interface**

**Problem**: Natural language is ambiguous and non-binding  
**Solution**: Standardized command symbols that create obligations

**Command Categories**:
```markdown
ğŸ›‘ BLOCKING COMMANDS (Cannot proceed until executed)
âš ï¸ WARNING COMMANDS (Strong guidance)
ğŸ¯ NAVIGATION COMMANDS (Cross-file routing)
ğŸ“Š EVIDENCE COMMANDS (Quantified validation)
ğŸ”„ PROGRESS COMMANDS (Status tracking)
ğŸš¨ VIOLATION DETECTION (Error prevention)
```

**Impact**: 10:1 token compression, 25-35% compliance improvement

---

### **Principle 4: Horizontal Task Decomposition**

**Problem**: Monolithic instructions cause context overflow  
**Solution**: Break into focused, single-responsibility modules

**Decomposition Pattern**:
```
Large Complex Task (2000 lines)
â†“
Phase Breakdown (8 phases Ã— 100 lines each)
â†“
Task Breakdown (30 tasks Ã— 50 lines each)
â†“
Optimal Context Utilization (15-25%)
```

---

### **Principle 5: Validation Gate Enforcement**

**Problem**: AI claims completion without thorough validation  
**Solution**: Programmatic quality gates at every boundary

**Gate Pattern**:
```markdown
ğŸ›‘ VALIDATE-GATE: [Phase Name]
- [ ] Criterion 1 âœ…/âŒ
- [ ] Criterion 2 âœ…/âŒ
- [ ] Criterion 3 âœ…/âŒ

ğŸš¨ FRAMEWORK-VIOLATION: If proceeding without âœ… on all criteria
```

---

## ğŸ—ï¸ **Framework Creation Process**

### **Phase 1: Domain Analysis (Week 1)**

**ğŸ›‘ EXECUTE-NOW: Answer these questions**

#### **1.1 Problem Definition**
```markdown
What complex task needs deterministic AI execution?
- Current pain points with ad-hoc approaches?
- What constitutes high-quality output?
- What are key success metrics?
```

#### **1.2 LLM Capability Assessment**
```markdown
Which aspects can AI handle well?
Which weaknesses need systematic compensation?
What are context and complexity constraints?
Where are highest-risk failure points?
```

#### **1.3 Complexity Estimation**
```markdown
How many logical phases? (Target: 5-10)
How many tasks per phase? (Target: 2-5)
Expected file count? (Target: â‰¤100 lines each)
What validation is needed? (Target: Automated gates)
```

ğŸ“Š **Deliverables**:
- [ ] Problem statement (1 page)
- [ ] Success criteria (quantified metrics)
- [ ] Failure risk assessment
- [ ] Estimated phase breakdown

---

### **Phase 2: Architecture Design (Week 1-2)**

#### **2.1 Three-Tier Structure Design**

**ğŸ¯ CREATE: Directory structure**
```
.agent-os/
â”œâ”€â”€ standards/
â”‚   â””â”€â”€ {framework-name}/
â”‚       â”œâ”€â”€ core/
â”‚       â”‚   â”œâ”€â”€ command-language-glossary.md     # Tier 1 (â‰¤100)
â”‚       â”‚   â””â”€â”€ framework-core.md                # Tier 2 (250-400)
â”‚       â”œâ”€â”€ phases/
â”‚       â”‚   â”œâ”€â”€ 0/
â”‚       â”‚   â”‚   â”œâ”€â”€ task-1-*.md                  # Tier 1 (â‰¤100)
â”‚       â”‚   â”‚   â”œâ”€â”€ task-2-*.md                  # Tier 1 (â‰¤100)
â”‚       â”‚   â”‚   â””â”€â”€ shared-analysis.md           # Tier 1 (â‰¤50)
â”‚       â”‚   â”œâ”€â”€ 1/
â”‚       â”‚   â”‚   â””â”€â”€ ...
â”‚       â”‚   â””â”€â”€ N/
â”‚       â”‚       â””â”€â”€ ...
â”‚       â”œâ”€â”€ methodology/                         # Optional Tier 2
â”‚       â”‚   â”œâ”€â”€ architecture.md                  # (200-500)
â”‚       â”‚   â””â”€â”€ patterns.md                      # (200-500)
â”‚       â”œâ”€â”€ FRAMEWORK_ENTRY_POINT.md             # Tier 1 (â‰¤100)
â”‚       â”œâ”€â”€ README.md                            # Tier 2 (200-500)
â”‚       â”œâ”€â”€ progress-tracking.md                 # Tier 1 (â‰¤100)
â”‚       â””â”€â”€ COMMON_PITFALLS.md                   # Tier 2 (200-400)
```

#### **2.2 Command Language Glossary**

**ğŸ›‘ EXECUTE-NOW: Copy template**
```bash
cp {source-repo}/.agent-os/standards/ai-assistant/code-generation/tests/v3/core/command-language-glossary.md \
   {target-repo}/.agent-os/standards/command-language-glossary.md
```

**âš ï¸ MUST-CUSTOMIZE**:
- Adapt command categories to domain
- Add domain-specific violation patterns
- Define framework-specific evidence types

#### **2.3 Discovery Flow Design**

**Navigation Architecture**:
```
User Request
  â†“
.cursorrules (Entry routing) [Optional]
  â†“
compliance-checking.md (Standards validation) [Optional]
  â†“
ai-assistant/README.md (Task routing hub) [Optional]
  â†“
{framework-name}/README.md (Methodology overview) [Tier 2]
  â†“
FRAMEWORK_ENTRY_POINT.md (Execution start) [Tier 1]
  â†“
phases/*/task-*.md (Systematic execution) [Tier 1]
```

ğŸ“Š **Deliverables**:
- [ ] Directory structure created
- [ ] Command glossary customized
- [ ] Discovery flow documented
- [ ] File size budget allocated

---

### **Phase 3: Phase Decomposition (Week 2)**

#### **3.1 Phase Identification**

**Pattern**: Each phase should:
- Have clear input/output boundaries
- Be independently validatable
- Take 15-60 minutes to execute
- Produce quantifiable evidence

**Example Phase Structures**:

**Analysis-Heavy Framework**:
```
0. Setup & Prerequisites
1. Source Discovery
2. Data Collection
3. Analysis & Categorization
4. Validation
5. Documentation
```

**Generation-Heavy Framework**:
```
0. Requirements Gathering
1. Pattern Analysis
2. Template Selection
3. Generation
4. Quality Validation
5. Integration Testing
```

#### **3.2 Task Breakdown Within Phases**

**ğŸ¯ CREATE: Task files (â‰¤100 lines each)**

**Template**:
```markdown
# Phase {N}: {Phase Name} - Task {M}: {Task Name}

âš ï¸ MUST-READ: [../core/command-language-glossary.md](../core/command-language-glossary.md)

---

## ğŸ¯ **Task Objective**

[1-2 sentence clear objective]

---

## ğŸ›‘ **Prerequisites**

ğŸ›‘ VALIDATE-GATE: Previous task complete
- [ ] [Specific prerequisite 1] âœ…/âŒ
- [ ] [Specific prerequisite 2] âœ…/âŒ

---

## ğŸ“‹ **Instructions**

### **Step 1: [Action]**
ğŸ›‘ EXECUTE-NOW: [Specific command]
ğŸ“Š EXPECTED-OUTPUT: [What success looks like]

### **Step 2: [Action]**
ğŸ›‘ EXECUTE-NOW: [Specific command]
ğŸ“Š COUNT-AND-DOCUMENT: [Quantifiable metric]

### **Step 3: [Action]**
ğŸ›‘ UPDATE-TABLE: Progress tracker with evidence

---

## ğŸ”„ **Progress Update**

ğŸ”„ UPDATE-STATUS: Phase {N}, Task {M} â†’ Complete
ğŸ”„ EVIDENCE-SUMMARY: [Specific evidence collected]

---

## ğŸ›‘ **Quality Gate**

ğŸ›‘ VALIDATE-GATE: Task completion
- [ ] All commands executed âœ…/âŒ
- [ ] Evidence documented âœ…/âŒ
- [ ] Output validated âœ…/âŒ

ğŸš¨ FRAMEWORK-VIOLATION: If proceeding without all âœ…

---

## ğŸ¯ **Next Step**

ğŸ¯ NEXT-MANDATORY: [phases/{N}/task-{M+1}.md] OR [phases/{N+1}/task-1.md]

---

**Task Version**: 1.0  
**Estimated Time**: {X} minutes  
**Automation**: {Manual/Semi-automated/Automated}
```

ğŸ“Š **Deliverables**:
- [ ] All phases identified (5-10 phases)
- [ ] All tasks created (â‰¤100 lines each)
- [ ] Dependencies mapped
- [ ] Time estimates provided

---

### **Phase 4: Validation Infrastructure (Week 2-3)**

#### **4.1 Progress Tracking Table**

**ğŸ¯ CREATE: progress-tracking.md**
```markdown
# {Framework Name} - Progress Tracking

**Instructions**: Copy this table to chat at framework start

---

## ğŸ“Š **Progress Table**

| Phase | Status | Tasks | Evidence | Quality Gate |
|-------|--------|-------|----------|--------------|
| 0 | â¸ï¸ | 0/4 | - | â¸ï¸ |
| 1 | â¸ï¸ | 0/6 | - | â¸ï¸ |
| 2 | â¸ï¸ | 0/3 | - | â¸ï¸ |
| ... | ... | ... | ... | ... |

**Status Key**:
- â¸ï¸ Not started
- ğŸ”„ In progress
- âœ… Complete
- âŒ Failed

**Quality Gate Key**:
- â¸ï¸ Not started
- â³ Pending validation
- âœ… Passed
- âŒ Failed

---

## ğŸ¯ **Current Phase**: {Will be updated}

## ğŸ“Š **Evidence Log**: {Will be populated}
```

#### **4.2 Automated Validation Scripts**

**Pattern**: Create validation scripts for quality gates

**Example (Go validator)**:
```go
// cmd/validate-{framework}/main.go
package main

func main() {
    result := ValidateFrameworkOutput(args)
    if result.HasErrors() {
        fmt.Printf("âŒ Validation Failed: %d errors\n", result.ErrorCount)
        os.Exit(1)
    }
    fmt.Println("âœ… Validation Passed")
    os.Exit(0)
}
```

**Example (Python validator)**:
```python
# scripts/validate_{framework}.py
def validate():
    errors = []
    # Validation logic
    if errors:
        print(f"âŒ Validation Failed: {len(errors)} errors")
        sys.exit(1)
    print("âœ… Validation Passed")
    sys.exit(0)
```

#### **4.3 Quality Gate Definitions**

**ğŸ¯ CREATE: Quality gate checklist for each phase**
```markdown
## Phase {N} Quality Gates

### Gate {N}.1: {Name}
**Validation Method**: {Automated script/Manual check}
**Command**: `{validation command}`
**Success Criteria**: {Specific, measurable}
**Failure Action**: {What to do if fails}

### Gate {N}.2: {Name}
...
```

ğŸ“Š **Deliverables**:
- [ ] Progress tracking template created
- [ ] Validation scripts implemented
- [ ] Quality gates defined for all phases
- [ ] Exit code enforcement (0=success, 1=failure)

---

### **Phase 5: Documentation (Week 3)**

#### **5.1 FRAMEWORK_ENTRY_POINT.md**

**Template** (â‰¤100 lines):
```markdown
# {Framework Name} - Entry Point

âš ï¸ MUST-READ: [core/command-language-glossary.md](core/command-language-glossary.md)

---

## ğŸ¯ **What Are You Doing?**

### **Option 1: NEW {Task Type}**
[Description]

ğŸ¯ NEXT-MANDATORY: [phases/0/task-1-*.md]

### **Option 2: UPDATE {Task Type}**
[Description]

ğŸ¯ NEXT-MANDATORY: [phases/0/task-2-*.md]

---

## ğŸ›‘ **Prerequisites**

ğŸ›‘ VALIDATE-GATE: Framework readiness
- [ ] Read command glossary âœ…/âŒ
- [ ] {Domain-specific prerequisite} âœ…/âŒ
- [ ] {Tool/environment setup} âœ…/âŒ

---

## ğŸ“Š **Framework Overview**

| Phase | Purpose | Duration |
|-------|---------|----------|
| 0 | {Purpose} | {Time} |
| 1 | {Purpose} | {Time} |
| ... | ... | ... |

---

## ğŸ¯ **Begin Framework**

ğŸ›‘ EXECUTE-NOW: Copy progress table to chat
âš ï¸ MUST-READ: [progress-tracking.md](progress-tracking.md)

ğŸ¯ NEXT-MANDATORY: Select your option above

---

**Framework Version**: 1.0
**Last Updated**: {Date}
```

#### **5.2 README.md**

**Template** (200-500 lines):
```markdown
# {Framework Name}

**Purpose**: {1-2 sentence description}

---

## ğŸ“‹ **Overview**

{Comprehensive framework description}

---

## ğŸ¯ **Purpose**

{Why this framework exists, what problems it solves}

---

## ğŸ“Š **Framework Structure**

{Directory tree, file organization}

---

## ğŸš€ **Quick Start**

1. Read command glossary
2. Execute entry point
3. Follow phase-by-phase guidance
4. Complete all quality gates

---

## ğŸ—ï¸ **Architecture**

{Three-tier architecture explanation}

---

## ğŸ“ˆ **Success Metrics**

{Expected improvements, quantified results}

---

## ğŸš¨ **Common Pitfalls**

{See COMMON_PITFALLS.md}

---

## ğŸ“ **Support**

{Where to get help, references}
```

#### **5.3 COMMON_PITFALLS.md**

**Template** (200-400 lines):
```markdown
# {Framework Name} - Common Pitfalls

---

## Pitfall 0: {Most Common/Critical}

### **Problem**
{Description}

### **Symptoms**
- {Observable indicator 1}
- {Observable indicator 2}

### **Root Cause**
{Why it happens}

### **Prevention**
{How to avoid}

### **Detection**
{How to identify}

### **Fix**
{How to correct}

---

[Repeat for each pitfall]
```

ğŸ“Š **Deliverables**:
- [ ] Entry point created (â‰¤100 lines)
- [ ] README comprehensive (200-500 lines)
- [ ] Common pitfalls documented
- [ ] Cross-references validated

---

### **Phase 6: Testing & Iteration (Week 3-4)**

#### **6.1 Pilot Execution**

**ğŸ›‘ EXECUTE-NOW: Test framework with AI**
```markdown
1. Start fresh chat session
2. Execute framework from entry point
3. Document ALL deviations, confusions, shortcuts
4. Record time per phase
5. Measure quality gate pass/fail rates
```

#### **6.2 Metrics Collection**

**ğŸ“Š QUANTIFY-RESULTS**:
```yaml
execution_consistency:
  total_runs: 10
  successful_completions: {count}
  average_quality_score: {score}
  
context_efficiency:
  average_file_size: {lines}
  largest_file: {lines}
  context_utilization: {percentage}
  
quality_enforcement:
  automated_gates: {count}
  manual_checks: {count}
  automation_percentage: {percentage}
```

#### **6.3 Iterative Refinement**

**Optimization Targets**:
- File size compliance: 95%+ files within tier limits
- Command language adoption: 80%+ instructions use commands
- Quality gate coverage: 100% phases have gates
- Execution consistency: 85%+ success rate

ğŸ“Š **Deliverables**:
- [ ] 10+ pilot runs completed
- [ ] Metrics collected and analyzed
- [ ] Refinements documented
- [ ] Success criteria met

---

## ğŸ“¦ **Portable Framework Package**

### **Minimal Viable Framework**

**Files Required** (Can start here):
```
{framework-name}/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ command-language-glossary.md     # Copy from template
â”œâ”€â”€ phases/
â”‚   â””â”€â”€ 0/
â”‚       â””â”€â”€ task-1-start.md              # First execution file
â”œâ”€â”€ FRAMEWORK_ENTRY_POINT.md             # Routing
â”œâ”€â”€ README.md                            # Overview
â””â”€â”€ progress-tracking.md                 # Status table
```

### **Full Framework Package**

**Complete Structure**:
```
{framework-name}/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ command-language-glossary.md
â”‚   â””â”€â”€ framework-core.md                # Optional deep dive
â”œâ”€â”€ phases/
â”‚   â”œâ”€â”€ 0/ ... N/                        # All execution phases
â”œâ”€â”€ methodology/                         # Optional Tier 2 context
â”‚   â”œâ”€â”€ architecture.md
â”‚   â””â”€â”€ patterns.md
â”œâ”€â”€ FRAMEWORK_ENTRY_POINT.md
â”œâ”€â”€ README.md
â”œâ”€â”€ progress-tracking.md
â”œâ”€â”€ COMMON_PITFALLS.md
â””â”€â”€ CHANGELOG.md
```

---

## ğŸš€ **Quick Start: New Framework**

### **Week 1 Checklist**
```markdown
ğŸ›‘ Day 1-2: Problem Analysis
- [ ] Define problem and success criteria
- [ ] Assess LLM capabilities vs constraints
- [ ] Estimate complexity (phases, tasks, files)

ğŸ›‘ Day 3-4: Architecture Design
- [ ] Create directory structure
- [ ] Copy/customize command glossary
- [ ] Design discovery flow

ğŸ›‘ Day 5: Phase Breakdown
- [ ] Identify 5-10 logical phases
- [ ] Map dependencies
- [ ] Allocate time budgets
```

### **Week 2 Checklist**
```markdown
ğŸ›‘ Day 1-3: Task Creation
- [ ] Create task files for Phase 0-2 (â‰¤100 lines each)
- [ ] Integrate command language
- [ ] Add validation gates

ğŸ›‘ Day 4-5: Validation Infrastructure
- [ ] Create progress tracking template
- [ ] Implement quality gate scripts
- [ ] Define success criteria
```

### **Week 3 Checklist**
```markdown
ğŸ›‘ Day 1-2: Documentation
- [ ] Write FRAMEWORK_ENTRY_POINT.md
- [ ] Write comprehensive README.md
- [ ] Start COMMON_PITFALLS.md

ğŸ›‘ Day 3-5: Testing
- [ ] Run 3-5 pilot executions
- [ ] Collect metrics
- [ ] Refine based on findings
```

### **Week 4: Polish & Deploy**
```markdown
ğŸ›‘ Day 1-3: Iteration
- [ ] Complete all remaining phase files
- [ ] Achieve 95%+ file size compliance
- [ ] Finalize quality gates

ğŸ›‘ Day 4-5: Validation
- [ ] Run 10+ executions
- [ ] Achieve 85%+ success rate
- [ ] Document lessons learned
```

---

## ğŸ“Š **Quality Assurance Checklist**

### **File Size Compliance**
- [ ] 95%+ Tier 1 files â‰¤100 lines
- [ ] 100% Tier 2 files â‰¤500 lines
- [ ] Tier 3 has no limits (output only)

### **Command Language Adoption**
- [ ] 80%+ instructions use command language
- [ ] All phase transitions have ğŸ¯ NEXT-MANDATORY
- [ ] All gates have ğŸ›‘ VALIDATE-GATE
- [ ] All evidence has ğŸ“Š COUNT-AND-DOCUMENT

### **Validation Coverage**
- [ ] 100% phases have quality gates
- [ ] 100% gates have automated validation
- [ ] 100% tasks have evidence requirements
- [ ] Progress table updated systematically

### **Navigation Completeness**
- [ ] Every file has explicit next-step routing
- [ ] Cross-file dependencies documented
- [ ] Discovery flow tested end-to-end
- [ ] No dead-end files (all have exit paths)

---

## ğŸ¯ **Success Metrics**

**Target Improvements** (vs ad-hoc approaches):

| Metric | Baseline | Target | Measurement |
|--------|----------|--------|-------------|
| **Execution Consistency** | 60-70% | 85-95% | Success rate across 10+ runs |
| **Context Efficiency** | 50-75% | 15-25% | Average context utilization |
| **Quality Enforcement** | Manual | 100% Automated | Percentage of gates automated |
| **Evidence Tracking** | Ad-hoc | 100% Systematic | Percentage of claims with evidence |

---

## ğŸ”— **References**

- **Source Methodologies**: `/python-sdk/.agent-os/standards/ai-assistant/`
  - `LLM-WORKFLOW-ENGINEERING-METHODOLOGY.md`
  - `DETERMINISTIC-LLM-OUTPUT-METHODOLOGY.md`
  - `code-generation/tests/v3/` (V3 framework implementation)

- **Case Studies**:
  - Python SDK V3 Test Generation Framework (80%+ success rate)
  - Provider Schema Extraction Framework (systematic schema extraction)

---

## ğŸ“ **Framework Maintenance**

### **Versioning**
- Increment version on breaking changes
- Document all updates in CHANGELOG.md
- Track framework-level improvements

### **Continuous Improvement**
- Collect metrics from every execution
- Document new pitfalls as discovered
- Refine file sizes based on usage
- Update command glossary as needed

### **Cross-Repository Transfer**
- Command glossary is portable (copy as-is)
- Three-tier architecture applies universally
- Validation pattern is reusable
- Progress tracking template is generic

---

**Guide Version**: 1.0  
**Last Updated**: 2025-10-02  
**Status**: Production-Ready  
**Transferability**: Universal (any domain, any repository)

