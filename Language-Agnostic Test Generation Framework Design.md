# Language-Agnostic Test Generation Framework Design

**Document Version:** 1.0  
**Date:** October 9, 2025  
**Author:** Agent OS Framework Design  
**Status:** Design Specification

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Design Philosophy](#design-philosophy)
3. [Architecture Overview](#architecture-overview)
4. [Universal Standards (Language-Agnostic)](#universal-standards-language-agnostic)
5. [Language-Specific Instructions](#language-specific-instructions)
6. [Workflow Construction Pattern](#workflow-construction-pattern)
7. [Implementation Roadmap](#implementation-roadmap)
8. [Example Instantiations](#example-instantiations)

---

## 1. Executive Summary

### Problem Statement

The V3 Test Generation Framework achieves 80%+ success rates for **Python** but is tightly coupled to Python-specific tooling (AST parsing, pytest, pylint, black). Organizations need test generation across multiple languages (JavaScript, Go, Java, Rust, TypeScript, C#) without duplicating the entire framework.

### Solution Design

Create a **three-layer architecture**:

```
Layer 1: Universal Test Generation Standards (Language-Agnostic)
    â†“
Layer 2: Language-Specific Instructions (Tooling & Patterns)
    â†“
Layer 3: Generated Workflow (Language-Specific, Project-Aware)
```

### Key Innovation

**Separation of Concerns:**
- **What to analyze** (universal) vs **How to analyze** (language-specific)
- **Test methodology** (universal) vs **Test frameworks** (language-specific)
- **Quality principles** (universal) vs **Quality tools** (language-specific)

### Success Metrics

- âœ… Single set of universal standards for all languages
- âœ… Language-specific instructions for 7+ languages
- âœ… 80%+ first-run success rate per language
- âœ… Automated workflow generation for detected languages
- âœ… <100 line file constraints maintained

---

## 2. Design Philosophy

### Core Principles

#### Principle 1: Universal Methodology, Language-Specific Tooling

```yaml
Universal (What):
  - "Analyze function signatures and parameters"
  - "Detect attribute access patterns"
  - "Map dependency graph"
  - "Plan branch coverage"

Language-Specific (How):
  Python:
    - "Use ast.parse() to extract function signatures"
    - "Use ast.walk() for attribute detection"
  JavaScript:
    - "Use @babel/parser to parse AST"
    - "Use espree for node traversal"
  Go:
    - "Use go/parser to parse syntax tree"
    - "Use go/ast for traversal"
```

#### Principle 2: Test Methodology is Universal

```yaml
Universal_Test_Concepts:
  unit_testing:
    - "Isolate code under test"
    - "Mock external dependencies"
    - "Verify behavior in isolation"
  
  integration_testing:
    - "Test component interactions"
    - "Use real dependencies"
    - "Verify end-to-end flows"
  
  coverage_principles:
    - "Branch coverage for all conditionals"
    - "Edge case identification"
    - "Error path testing"
  
  quality_gates:
    - "100% test pass rate"
    - "Static analysis with zero errors"
    - "Consistent formatting"
    - "Type safety validation"
```

#### Principle 3: Eight-Phase Analysis is Universal

```yaml
Phase_Methodology:
  phase_0: "Environment setup and path selection"
  phase_1: "Method/function verification"
  phase_2: "Logging/output analysis"
  phase_3: "Dependency mapping"
  phase_4: "Usage pattern analysis"
  phase_5: "Coverage planning"
  phase_6: "Pre-generation validation"
  phase_7: "Test generation and metrics"
  phase_8: "Quality enforcement"

# Each phase WHAT is universal
# Each phase HOW is language-specific
```

#### Principle 4: Quality Standards are Universal

```yaml
Universal_Quality_Targets:
  test_execution:
    requirement: "100% pass rate"
    rationale: "Generated tests must work"
  
  static_analysis:
    requirement: "Perfect score (language-specific scale)"
    rationale: "Consistent code quality"
  
  type_safety:
    requirement: "Zero type errors"
    rationale: "Type correctness validation"
  
  formatting:
    requirement: "100% compliant with formatter"
    rationale: "Consistent style"
  
  coverage:
    unit_tests: "90%+ line and branch"
    integration_tests: "Functional flow coverage"
```

---

## 3. Architecture Overview

### Three-Layer System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1: Universal Standards (Language-Agnostic)           â”‚
â”‚  Location: universal/standards/testing/                     â”‚
â”‚                                                              â”‚
â”‚  - test-generation-methodology.md                           â”‚
â”‚  - analysis-phase-framework.md                              â”‚
â”‚  - path-system-standards.md (unit vs integration)           â”‚
â”‚  - quality-gate-requirements.md                             â”‚
â”‚  - evidence-tracking-standards.md                           â”‚
â”‚  - coverage-planning-principles.md                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 2: Language-Specific Instructions                    â”‚
â”‚  Location: language-instructions/                           â”‚
â”‚                                                              â”‚
â”‚  - python-test-generation.md                                â”‚
â”‚  - javascript-test-generation.md                            â”‚
â”‚  - go-test-generation.md                                    â”‚
â”‚  - java-test-generation.md                                  â”‚
â”‚  - typescript-test-generation.md                            â”‚
â”‚  - rust-test-generation.md                                  â”‚
â”‚  - csharp-test-generation.md                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 3: Generated Workflow (Language + Project Specific)  â”‚
â”‚  Location: .agent-os/workflows/test_generation_{lang}_v1/   â”‚
â”‚                                                              â”‚
â”‚  Generated by LLM using:                                     â”‚
â”‚  - Universal standards (Layer 1)                            â”‚
â”‚  - Language instructions (Layer 2)                          â”‚
â”‚  - Detected project patterns (runtime analysis)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### File Organization

```
universal/standards/testing/
â”œâ”€â”€ README.md                              # Overview
â”œâ”€â”€ test-generation/
â”‚   â”œâ”€â”€ methodology.md                     # 8-phase overview
â”‚   â”œâ”€â”€ phase-0-setup.md                   # Universal setup
â”‚   â”œâ”€â”€ phase-1-function-analysis.md       # Function/method analysis
â”‚   â”œâ”€â”€ phase-2-output-analysis.md         # Logging/output patterns
â”‚   â”œâ”€â”€ phase-3-dependency-analysis.md     # Dependency mapping
â”‚   â”œâ”€â”€ phase-4-usage-patterns.md          # Call pattern analysis
â”‚   â”œâ”€â”€ phase-5-coverage-planning.md       # Coverage strategy
â”‚   â”œâ”€â”€ phase-6-validation.md              # Pre-generation checks
â”‚   â”œâ”€â”€ phase-7-generation.md              # Test generation
â”‚   â”œâ”€â”€ phase-8-quality-gates.md           # Quality enforcement
â”‚   â”œâ”€â”€ path-system.md                     # Unit vs integration
â”‚   â”œâ”€â”€ evidence-tracking.md               # Progress tables
â”‚   â””â”€â”€ quality-standards.md               # Quality targets

language-instructions/
â”œâ”€â”€ python-test-generation.md              # Python tooling
â”œâ”€â”€ javascript-test-generation.md          # JS/Jest/Mocha
â”œâ”€â”€ typescript-test-generation.md          # TS + type checking
â”œâ”€â”€ go-test-generation.md                  # Go testing pkg
â”œâ”€â”€ java-test-generation.md                # JUnit/TestNG
â”œâ”€â”€ rust-test-generation.md                # Cargo test
â””â”€â”€ csharp-test-generation.md              # xUnit/NUnit
```

---

## 4. Universal Standards (Language-Agnostic)

### 4.1 Test Generation Methodology

**File:** `universal/standards/testing/test-generation/methodology.md`

**Purpose:** Define the universal 8-phase approach to test generation

**Content Structure:**

```markdown
# Test Generation Methodology (Universal)

## Overview

This standard defines the systematic approach to generating high-quality tests for **any programming language**. The methodology consists of 8 sequential phases that build comprehensive understanding before generation.

## Core Principles

### Principle 1: Deep Analysis Before Generation

âŒ Surface-level analysis â†’ Incomplete tests â†’ Failures
âœ… Deep analysis â†’ Comprehensive tests â†’ Success

### Principle 2: Path-Based Strategies

- **Unit Path**: Mock external dependencies, test in isolation
- **Integration Path**: Real dependencies, end-to-end validation
- Path selected at Phase 0, enforced throughout

### Principle 3: Evidence-Based Progress

- Every phase produces quantified evidence
- Progress tracked in structured tables
- Automated validation with exit codes

## Eight-Phase Framework

### Phase 0: Environment Setup and Path Selection

**Universal Requirements:**
1. Validate development environment
2. Analyze target file complexity
3. Select test path (unit OR integration)
4. Initialize progress tracking

**Language-Specific Implementation:**
- Environment validation commands differ per language
- Complexity metrics use language-specific tools
- See: `language-instructions/{language}-test-generation.md`

### Phase 1: Function/Method Analysis

**Universal Requirements:**
1. Extract all function/method signatures
2. Document parameter lists and types
3. Identify return types
4. Detect attribute/field access patterns
5. Map function call patterns

**Analysis Output:**
- Function inventory with signatures
- Parameter count and types
- All accessed attributes
- Mock completeness requirements (unit path)

**Language-Specific Implementation:**
- Python: AST parsing with ast.parse()
- JavaScript: @babel/parser or espree
- Go: go/parser and go/ast
- See: Phase 1 section in language instructions

### Phase 2: Output Analysis

**Universal Requirements:**
1. Identify all logging/output statements
2. Classify output levels (debug, info, error, etc.)
3. Determine output strategy per path
   - Unit: Mock output to prevent side effects
   - Integration: Use real output for validation

**Language-Specific Implementation:**
- Python: grep for "logger" or AST analysis
- JavaScript: console.log/winston/pino detection
- Go: fmt.Print*/log package analysis

### Phase 3: Dependency Analysis

**Universal Requirements:**
1. Map all external dependencies
2. Map all internal module dependencies
3. Identify configuration dependencies
4. Document I/O dependencies (file, network, database)

**Dependency Strategy:**
- Unit Path: ALL external dependencies mocked
- Integration Path: Real dependencies, selective mocking

**Language-Specific Implementation:**
- Python: import analysis + module inspection
- JavaScript: require()/import analysis
- Go: import path analysis
- Java: import + package analysis

### Phase 4: Usage Pattern Analysis

**Universal Requirements:**
1. Analyze all function call sites
2. Document control flow patterns (if/switch/loops)
3. Map error handling patterns
4. Identify state management patterns

**Language-Specific Implementation:**
- Varies by language syntax and idioms
- See language instructions for parsing strategies

### Phase 5: Coverage Planning

**Universal Requirements:**
1. Identify all conditional branches
2. Plan edge case testing
3. Document error paths
4. Establish coverage targets
   - Unit: 90%+ line and branch coverage
   - Integration: Functional flow coverage

**Language-Specific Implementation:**
- Coverage tools differ per language
- See language instructions for coverage measurement

### Phase 6: Pre-Generation Validation

**Universal Requirements:**
1. Verify all technical prerequisites
2. Validate complete analysis chain (Phases 1-5)
3. Confirm path-specific strategy
4. Prepare quality tool integration

**Language-Specific Implementation:**
- Import/module validation commands
- Signature verification approaches
- See language instructions

### Phase 7: Test Generation and Metrics

**Universal Requirements:**
1. Generate comprehensive test file
2. Execute tests and collect metrics
3. Measure coverage
4. Run static analysis
5. Check formatting

**Language-Specific Implementation:**
- Test framework patterns (pytest, Jest, go test, JUnit)
- Assertion libraries
- Fixture patterns
- See language instructions and templates

### Phase 8: Quality Enforcement

**Universal Requirements:**
1. 100% test pass rate (zero failures)
2. Perfect static analysis score
3. Zero type errors (if typed language)
4. 100% formatter compliance
5. Coverage targets met

**Enforcement:**
- Automated validation script with exit code 0
- Zero tolerance for quality failures
- Language-specific quality tools
- See: quality-gate-requirements.md

## Success Criteria

- âœ… All 8 phases completed with evidence
- âœ… Progress table shows all phases complete
- âœ… Path strategy followed consistently
- âœ… Quality gates passed with automation
- âœ… 80%+ first-run success rate target

## See Also

- [Path System Standards](path-system.md)
- [Evidence Tracking](evidence-tracking.md)
- [Quality Standards](quality-standards.md)
- Language-specific instructions in `/language-instructions/`
```

### 4.2 Path System Standards

**File:** `universal/standards/testing/test-generation/path-system.md`

**Purpose:** Define unit vs integration testing strategies universally

**Content Structure:**

```markdown
# Path System Standards (Universal)

## Overview

The path system provides two distinct testing strategies applicable to **any programming language**:

1. **Unit Path**: Isolation through mocking
2. **Integration Path**: End-to-end validation

## Universal Path Principles

### Unit Testing Path

**Core Strategy:** Isolate code under test by mocking external dependencies

**Universal Characteristics:**
- Mock all external APIs (HTTP, database, file system)
- Mock all external libraries
- Execute real production code
- High determinism
- Fast execution
- 90%+ coverage target

**What Gets Mocked (Universal):**
```yaml
External_APIs:
  - HTTP clients
  - Database connections
  - Message queues
  - External services

External_Libraries:
  - Third-party packages
  - Standard library I/O
  - Time/date functions
  - Random number generators

Configuration:
  - Environment variables
  - Config files
  - Secrets/credentials

I/O_Operations:
  - File system access
  - Network sockets
  - Standard input/output
```

**What Gets Executed (Universal):**
```yaml
Code_Under_Test:
  - All functions/methods being tested
  - Internal logic and algorithms
  - Control flow
  - Error handling
```

**Critical Insight:**
```
âŒ WRONG: Mock the function under test â†’ 0% coverage
âœ… CORRECT: Mock the dependencies â†’ Execute code â†’ Coverage!
```

### Integration Testing Path

**Core Strategy:** Validate real system interactions end-to-end

**Universal Characteristics:**
- Use real external dependencies
- Validate actual system behavior
- Verify backend integration
- Subject to system state
- Slower execution
- Functional coverage focus

**What Stays Real (Universal):**
```yaml
Real_Dependencies:
  - Actual API clients
  - Real database connections
  - Live external services (test environment)
  - Actual message queues

Real_Configuration:
  - Test environment configs
  - Test credentials
  - Real connection strings

Real_I/O:
  - Actual file operations (test directories)
  - Real network calls
```

**What Gets Mocked (Minimal):**
```yaml
Test_Specific:
  - Test data generation
  - Cleanup helpers
  - Time manipulation (sometimes)
```

## Path Selection Decision Tree

```yaml
Choose_Unit_When:
  - Testing single module/class
  - Need fast feedback
  - Require high determinism
  - Want coverage metrics
  - External dependencies costly/slow

Choose_Integration_When:
  - Testing multiple components
  - Need end-to-end validation
  - Verifying API contracts
  - System behavior critical
  - Backend verification required
```

## Path Enforcement

### Phase 0: Path Lock

- Path selected at Phase 0
- Selection documented in progress table
- **Cannot change mid-execution**
- All subsequent phases follow selected path

### Violation Detection

```yaml
Path_Mixing_Violations:
  indicator: "Unit mocks in integration test"
  response: "ðŸš¨ FRAMEWORK-VIOLATION - Path locked at Phase 0"
  
  indicator: "Integration fixtures in unit test"
  response: "ðŸš¨ FRAMEWORK-VIOLATION - Must use unit mocks"
```

## Language-Specific Mocking

### Mock Patterns by Language

**Python:**
- unittest.mock (Mock, patch, MagicMock)
- pytest-mock
- responses (HTTP mocking)

**JavaScript:**
- Jest mocking
- Sinon.js
- nock (HTTP)

**Go:**
- Interface-based mocking
- gomock
- testify/mock

**Java:**
- Mockito
- EasyMock
- PowerMock

**See:** Language-specific instructions for detailed patterns

## Success Patterns

### Unit Path Success:
```yaml
Analysis: "All dependencies identified"
Strategy: "Complete mock suite prepared"
Generation: "Mocks + real code execution"
Validation: "90%+ coverage achieved"
```

### Integration Path Success:
```yaml
Analysis: "All integration points mapped"
Strategy: "Real dependencies configured"
Generation: "Real calls + backend verification"
Validation: "Functional flows complete"
```

## See Also

- [Test Generation Methodology](methodology.md)
- [Quality Standards](quality-standards.md)
- Language-specific mock patterns in `/language-instructions/`
```

### 4.3 Evidence Tracking Standards

**File:** `universal/standards/testing/test-generation/evidence-tracking.md`

**Purpose:** Define progress tracking and evidence requirements

**Content Structure:**

```markdown
# Evidence Tracking Standards (Universal)

## Overview

Evidence tracking ensures systematic execution and prevents vague claims. Progress is tracked through structured tables updated after each phase.

## Universal Progress Table

### Template Structure

| Phase | Status | Evidence | Commands | Validation | Gate |
|-------|--------|----------|----------|------------|------|
| 0: Setup | âŒ | None | 0/N | Manual | âŒ |
| 1: Function Analysis | âŒ | None | 0/N | Manual | âŒ |
| 2: Output Analysis | âŒ | None | 0/N | Manual | âŒ |
| 3: Dependency Analysis | âŒ | None | 0/N | Manual | âŒ |
| 4: Usage Patterns | âŒ | None | 0/N | Manual | âŒ |
| 5: Coverage Planning | âŒ | None | 0/N | Manual | âŒ |
| 6: Pre-Generation | âŒ | None | 0/N | Manual | âŒ |
| 7: Generation + Metrics | âŒ | None | 0/1 | JSON | âŒ |
| 8: **Quality Gates** | âŒ | None | 0/1 | **EXIT CODE 0** | âŒ |

## Evidence Requirements

### Quantified Evidence (Not Vague)

âŒ **Unacceptable Evidence:**
- "Analysis complete"
- "Several functions found"
- "Dependencies identified"
- "Tests passing"

âœ… **Acceptable Evidence:**
- "15 functions found: [list signatures]"
- "8 external dependencies: requests, os, sys..."
- "23 attribute accesses detected"
- "Tests: 18/18 passing (100%)"

### Evidence by Phase

**Phase 0:**
```yaml
Required:
  - Environment validation output
  - Target file metrics (lines, functions)
  - Path selection (unit OR integration)
  - Progress table initialized
```

**Phase 1:**
```yaml
Required:
  - Function count with signatures
  - Parameter lists and types
  - Attribute access patterns (count + list)
  - Mock completeness requirements
```

**Phase 2:**
```yaml
Required:
  - Output statement count
  - Output levels identified
  - Path-specific strategy
```

**Phase 3:**
```yaml
Required:
  - External dependency list
  - Internal dependency list
  - Configuration dependency list
  - Mock strategy documented
```

**Phase 4:**
```yaml
Required:
  - Function call pattern count
  - Control flow branch count
  - Error handling pattern count
```

**Phase 5:**
```yaml
Required:
  - Conditional branch count
  - Edge case list
  - Coverage target plan
```

**Phase 6:**
```yaml
Required:
  - Technical prerequisite validation
  - Analysis completeness checklist
  - Path strategy confirmation
```

**Phase 7:**
```yaml
Required:
  - Test execution results (pass/fail counts)
  - Coverage percentage
  - Static analysis score
  - Type checking results
  - Formatting status
```

**Phase 8:**
```yaml
Required:
  - Automated script execution output
  - Exit code (must be 0)
  - All quality gate results
```

## Command Execution Tracking

### Command Count Format

```
Commands: X/Y executed
Where:
  X = Commands executed
  Y = Total commands required
```

### Command Output Requirements

ðŸ›‘ **PASTE-OUTPUT:** Command results must be pasted, not summarized

Example:
```
âœ… Correct:
Command: wc -l src/tracer.py
Output:
  312 src/tracer.py

âŒ Wrong:
Command: wc -l src/tracer.py
Output: File has several hundred lines
```

## Status Indicators

### Phase Status

- âŒ Not started
- ðŸ”„ In progress
- âœ… Complete with evidence
- âš ï¸ Blocked (with reason)

### Gate Status

- âŒ Not validated
- âš ï¸ Validation in progress
- âœ… Passed
- ðŸš« Failed (with details)

## Validation Requirements

### Manual Validation (Phases 0-6)

```yaml
Requirements:
  - Evidence documented in table
  - Commands executed with output
  - Findings quantified
  - Ready to proceed to next phase
```

### JSON Validation (Phase 7)

```yaml
Requirements:
  - Structured metrics in JSON format
  - All metrics populated
  - No placeholders
  - See: phase-7-generation.md
```

### Automated Validation (Phase 8)

```yaml
Requirements:
  - Script execution output pasted
  - Exit code explicitly stated
  - Must be exit code 0
  - All gates documented
```

## Violation Detection

### Evidence Gaps

```yaml
Indicator: "Phase marked complete without quantified evidence"
Response: "ðŸš¨ EVIDENCE-GAP: Provide specific counts and findings"
```

### Phase Skipping

```yaml
Indicator: "Moving to Phase X without Phase X-1 evidence"
Response: "ðŸš¨ FRAMEWORK-VIOLATION: Complete previous phase"
```

### Vague Claims

```yaml
Indicator: "Tests passing" without pass/fail counts
Response: "ðŸ“Š COUNT-AND-DOCUMENT: Provide exact test counts"
```

## See Also

- [Test Generation Methodology](methodology.md)
- [Quality Standards](quality-standards.md)
- [Command Language](../../../meta-framework/command-language.md)
```

### 4.4 Quality Gate Requirements

**File:** `universal/standards/testing/test-generation/quality-standards.md`

**Purpose:** Define universal quality targets and enforcement

**Content Structure:**

```markdown
# Quality Standards (Universal)

## Overview

Quality standards apply to **all generated tests** regardless of programming language. These are non-negotiable requirements enforced through automated validation.

## Universal Quality Targets

### 1. Test Execution (100% Pass Rate)

**Requirement:**
```yaml
target: "100% test pass rate"
tolerance: "Zero failed tests"
rationale: "Generated tests must work correctly"
```

**Measurement:**
```
All test frameworks report pass/fail counts
Requirement: pass_count == total_count AND fail_count == 0
```

**Language-Specific Frameworks:**
- Python: pytest, unittest
- JavaScript: Jest, Mocha, Jasmine
- Go: go test
- Java: JUnit, TestNG
- Rust: cargo test
- C#: xUnit, NUnit

### 2. Static Analysis (Perfect Score)

**Requirement:**
```yaml
target: "Perfect static analysis score"
tolerance: "Zero warnings/errors"
rationale: "Consistent code quality"
```

**Language-Specific Scales:**
- Python: Pylint 10.0/10
- JavaScript: ESLint 0 errors/warnings
- Go: go vet 0 issues
- Java: CheckStyle 0 violations
- Rust: clippy 0 warnings
- TypeScript: TSLint/ESLint 0 issues
- C#: StyleCop 0 violations

### 3. Type Safety (Zero Type Errors)

**Requirement:**
```yaml
target: "Zero type errors"
tolerance: "None"
rationale: "Type correctness validation"
applicable: "Typed languages and optional typing"
```

**Language-Specific Type Checkers:**
- Python: mypy (if type hints used)
- JavaScript: Flow (if used)
- TypeScript: tsc
- Go: Built-in type system
- Java: javac
- Rust: rustc
- C#: Built-in type system

### 4. Code Formatting (100% Compliant)

**Requirement:**
```yaml
target: "100% formatter compliance"
tolerance: "Zero formatting issues"
rationale: "Consistent style"
```

**Language-Specific Formatters:**
- Python: Black
- JavaScript: Prettier
- Go: gofmt
- Java: google-java-format
- Rust: rustfmt
- TypeScript: Prettier
- C#: dotnet format

### 5. Code Coverage

**Requirements:**
```yaml
unit_tests:
  line_coverage: "90%+ minimum"
  branch_coverage: "90%+ minimum"
  rationale: "Comprehensive testing"

integration_tests:
  requirement: "Functional flow coverage"
  metric: "All integration points validated"
  rationale: "End-to-end validation"
```

**Language-Specific Coverage Tools:**
- Python: coverage.py, pytest-cov
- JavaScript: Istanbul, Jest coverage
- Go: go test -cover
- Java: JaCoCo
- Rust: tarpaulin
- TypeScript: Istanbul, Jest
- C#: coverlet

## Quality Enforcement System

### Phase 8: Automated Validation

**Validation Script:**
```
Language-specific script that:
1. Runs all tests â†’ Checks 100% pass rate
2. Runs static analyzer â†’ Checks perfect score
3. Runs type checker â†’ Checks zero errors
4. Runs formatter â†’ Checks compliance
5. Runs coverage tool â†’ Checks targets

Exit code 0: All gates passed âœ…
Exit code 1: One or more gates failed âŒ
```

**Script Naming:**
- Python: `validate-python-test-quality.py`
- JavaScript: `validate-javascript-test-quality.js`
- Go: `validate-go-test-quality.sh`
- Etc.

### Zero Tolerance Enforcement

```yaml
ðŸš¨ ABSOLUTE REQUIREMENTS:

test_pass_rate: "100% (not 95%, not 99%)"
static_analysis: "Perfect (not 9.5/10)"
type_errors: "0 (not 2, not 'just warnings')"
formatting: "100% (not 'mostly formatted')"
coverage: "90%+ for unit (not 85%)"

ðŸš¨ FRAMEWORK-VIOLATION: Declaring success with ANY failure
```

### Quality Gate Output

**Required Format:**
```
=== Quality Validation Results ===

âœ… Test Execution: X/X tests passed (100%)
âœ… Coverage: X.X% lines, X.X% branches
âœ… Static Analysis: {Perfect score in language scale}
âœ… Type Checking: 0 errors
âœ… Formatting: Compliant

=== ALL QUALITY GATES PASSED ===
Exit code: 0
```

## Failure Remediation

### Failure Categories

**1. Test Failures:**
```yaml
symptom: "Tests failing on execution"
causes:
  - Incorrect mock attributes
  - Wrong function signatures
  - Missing dependencies
  - Logic errors
remediation: "Return to Phase 1-4 analysis"
```

**2. Coverage Failures:**
```yaml
symptom: "Coverage below 90%"
causes:
  - Missing test cases
  - Uncovered branches
  - Edge cases not tested
remediation: "Return to Phase 5 coverage planning"
```

**3. Static Analysis Failures:**
```yaml
symptom: "Linter warnings/errors"
causes:
  - Style violations
  - Code smells
  - Unused imports
remediation: "Fix issues, re-validate"
```

**4. Type Errors:**
```yaml
symptom: "Type checker errors"
causes:
  - Incorrect type annotations
  - Type mismatches
  - Missing type definitions
remediation: "Fix types, re-validate"
```

## Language-Specific Quality Configuration

### Configuration Files

Each language may have quality tool configuration:

**Python:**
- `pyproject.toml`: Black, Pylint, MyPy config
- `.pylintrc`: Pylint settings
- `mypy.ini`: Type checking config

**JavaScript:**
- `.eslintrc`: ESLint rules
- `.prettierrc`: Prettier config
- `jest.config.js`: Jest settings

**Go:**
- `.golangci.yml`: Linter configuration

**See:** Language-specific instructions for configuration details

## Success Metrics

### Target Success Rate

```yaml
framework_target:
  first_run_success: "80%+ per language"
  definition: "Tests pass all quality gates on first execution"
  measurement: "Across diverse modules/projects"
```

### Quality Consistency

```yaml
consistency_target:
  perfect_scores: "100% of successful generations"
  rationale: "Automated validation ensures consistency"
```

## See Also

- [Test Generation Methodology](methodology.md)
- [Phase 8: Quality Enforcement](phase-8-quality-gates.md)
- Language-specific quality tools in `/language-instructions/`
```

---

## 5. Language-Specific Instructions

### 5.1 Instruction Template

**Purpose:** Language-specific instructions tell the LLM **HOW** to apply universal standards to a specific language.

**Template File:** `language-instructions/{language}-test-generation.md`

**Structure:**

```markdown
# {Language} Test Generation Instructions

**For Cursor Agent: When generating tests for {Language}, use these instructions to apply universal test generation standards with {Language}-specific tooling.**

---

## Overview

This document maps **universal test generation standards** to **{Language}-specific implementation**.

### Universal Standards Reference

Read these universal standards:
- `universal/standards/testing/test-generation/methodology.md`
- `universal/standards/testing/test-generation/path-system.md`
- `universal/standards/testing/test-generation/quality-standards.md`

### {Language} Context

This file provides:
- {Language}-specific analysis tools
- {Language} testing frameworks
- {Language} mocking patterns
- {Language} quality tools
- {Language} code examples

---

## Phase 0: Environment Setup ({Language})

### Environment Validation

**Universal Requirement:** Validate development environment

**{Language} Implementation:**

```bash
# Check {Language} installation
{language_version_command}

# Check package manager
{package_manager_command}

# Check build tool
{build_tool_command}

# Verify workspace
pwd
ls -la {relevant_paths}
```

**Example:**
```
$ python --version
Python 3.11.5

$ which pytest
/usr/local/bin/pytest
```

### Target File Analysis

**Universal Requirement:** Analyze target file complexity

**{Language} Implementation:**

```bash
# Line count
wc -l {target_file}

# Function/method count
{language_specific_function_count_command}

# Dependency count
{language_specific_dependency_count_command}
```

### Path Selection

**Universal Requirement:** Select unit OR integration path

**Decision Factors:**
- See universal path-system.md
- {Language}-specific framework availability
- Project testing patterns

---

## Phase 1: Function/Method Analysis ({Language})

### Universal Requirements

From `methodology.md` Phase 1:
1. Extract all function/method signatures
2. Document parameters and types
3. Identify return types
4. Detect attribute/field access
5. Map function calls

### {Language} Implementation

#### AST-Based Signature Extraction

**Tool:** {Language parsing tool}

**Command:**
```{language}
{language_specific_ast_parsing_script}
```

**Example Output:**
```
function_name(param1: Type1, param2: Type2) -> ReturnType at line X
```

#### Attribute Access Detection

**Pattern:** {Language attribute syntax}

**Command:**
```bash
{language_specific_attribute_detection_command}
```

#### Function Call Analysis

**Command:**
```bash
{language_specific_function_call_detection}
```

### Mock Completeness (Unit Path)

**Universal Principle:** All accessed attributes must be mocked

**{Language} Pattern:**
```{language}
{language_specific_mock_attribute_pattern}
```

---

## Phase 2: Output Analysis ({Language})

### Universal Requirements

1. Identify logging/output statements
2. Classify output levels
3. Determine path strategy

### {Language} Implementation

#### Logging Framework Detection

**Common {Language} Logging:**
- {logging_framework_1}
- {logging_framework_2}
- {logging_framework_3}

**Detection Command:**
```bash
{language_specific_logging_detection}
```

#### Output Mocking (Unit Path)

**Pattern:**
```{language}
{language_specific_output_mocking_pattern}
```

---

## Phase 3: Dependency Analysis ({Language})

### Universal Requirements

1. Map external dependencies
2. Map internal dependencies
3. Identify configuration
4. Document I/O dependencies

### {Language} Implementation

#### Import/Dependency Analysis

**Command:**
```bash
{language_specific_import_analysis}
```

#### External vs Internal Classification

**{Language} Conventions:**
- External: {external_dependency_indicators}
- Internal: {internal_dependency_indicators}

#### Mock Strategy (Unit Path)

**External Dependency Mocking:**
```{language}
{language_specific_external_dependency_mock_pattern}
```

**Internal Dependency Handling:**
```{language}
{language_specific_internal_dependency_pattern}
```

---

## Phase 4: Usage Pattern Analysis ({Language})

### {Language}-Specific Patterns

#### Control Flow

**{Language} Conditionals:**
```bash
{language_specific_control_flow_detection}
```

#### Error Handling

**{Language} Error Patterns:**
- {error_pattern_1}
- {error_pattern_2}

**Detection:**
```bash
{language_specific_error_handling_detection}
```

---

## Phase 5: Coverage Planning ({Language})

### Coverage Tool

**{Language} Coverage Tool:** {coverage_tool_name}

**Usage:**
```bash
{coverage_tool_command}
```

**Output Interpretation:**
```
{coverage_output_example}
```

### Branch Identification

**Command:**
```bash
{language_specific_branch_detection}
```

---

## Phase 6: Pre-Generation Validation ({Language})

### Import Validation

**Command:**
```{language}
{language_specific_import_validation}
```

### Signature Verification

**Command:**
```{language}
{language_specific_signature_verification}
```

---

## Phase 7: Test Generation ({Language})

### Testing Frameworks

**Recommended: {primary_test_framework}**

| Framework | Use Case | Status |
|-----------|----------|--------|
| {framework_1} | {use_case_1} | âœ… Recommended |
| {framework_2} | {use_case_2} | âš ï¸ Alternative |

### Unit Test Template ({Language})

```{language}
{language_specific_unit_test_template}
```

### Integration Test Template ({Language})

```{language}
{language_specific_integration_test_template}
```

### Assertion Patterns

```{language}
{language_specific_assertion_patterns}
```

### Fixture Patterns

```{language}
{language_specific_fixture_patterns}
```

### Mocking Patterns

#### Mock External APIs

```{language}
{language_specific_api_mocking}
```

#### Mock Database

```{language}
{language_specific_database_mocking}
```

#### Mock File System

```{language}
{language_specific_filesystem_mocking}
```

---

## Phase 8: Quality Enforcement ({Language})

### Quality Tools

**Static Analyzer:** {static_analyzer}
- Command: `{static_analyzer_command}`
- Target: {perfect_score_definition}

**Type Checker:** {type_checker}
- Command: `{type_checker_command}`
- Target: 0 errors

**Formatter:** {formatter}
- Command: `{formatter_command}`
- Target: 100% compliant

**Coverage:** {coverage_tool}
- Command: `{coverage_command}`
- Target: 90%+ for unit

### Validation Script

**Script:** `scripts/validate-{language}-test-quality.{ext}`

```{language}
{validation_script_content}
```

**Usage:**
```bash
{validation_script_usage}
```

**Required Output:**
```
=== Quality Validation Results ===

âœ… Test Execution: X/X tests passed (100%)
âœ… Coverage: X.X% lines, X.X% branches  
âœ… Static Analysis: {perfect_score}
âœ… Type Checking: 0 errors
âœ… Formatting: Compliant

=== ALL QUALITY GATES PASSED ===
Exit code: 0
```

---

## Project Context Integration

### Framework Detection

**Detect {Language} Frameworks:**
- {framework_1}: Check for {indicator_1}
- {framework_2}: Check for {indicator_2}

### Configuration Files

**Detect:**
- {config_file_1}: {purpose}
- {config_file_2}: {purpose}

**Generate if missing:**
```{language}
{config_file_template}
```

---

## Common Patterns

### {Language}-Specific Test Patterns

#### Pattern 1: {pattern_name}
```{language}
{pattern_code}
```

#### Pattern 2: {pattern_name}
```{language}
{pattern_code}
```

---

## Troubleshooting

### Common Issues

**Issue 1: {common_issue_1}**
- Symptom: {symptom}
- Cause: {cause}
- Solution: {solution}

**Issue 2: {common_issue_2}**
- Symptom: {symptom}
- Cause: {cause}
- Solution: {solution}

---

## See Also

- [Universal Test Generation Methodology](../universal/standards/testing/test-generation/methodology.md)
- [Path System Standards](../universal/standards/testing/test-generation/path-system.md)
- [Quality Standards](../universal/standards/testing/test-generation/quality-standards.md)
```

---

## 6. Workflow Construction Pattern

### 6.1 Automated Workflow Generation

**Process:**

```yaml
Step_1_Language_Detection:
  input: "Target project directory"
  detection:
    - Check for language-specific files
    - Analyze package manager files
    - Detect build tools
  output: "Detected language(s)"

Step_2_Read_Universal_Standards:
  input: "universal/standards/testing/test-generation/"
  action: "Load all universal standard files"
  output: "Universal methodology understanding"

Step_3_Read_Language_Instructions:
  input: "language-instructions/{detected_language}-test-generation.md"
  action: "Load language-specific implementation guide"
  output: "Language-specific tooling knowledge"

Step_4_Analyze_Project:
  input: "Target project"
  actions:
    - Detect testing frameworks
    - Identify quality tools
    - Find configuration files
    - Analyze existing test patterns
  output: "Project-specific context"

Step_5_Generate_Workflow:
  input:
    - Universal standards (Step 2)
    - Language instructions (Step 3)
    - Project context (Step 4)
  action: "Generate language-specific workflow"
  output: ".agent-os/workflows/test_generation_{language}_v1/"

Step_6_Create_Workflow_Metadata:
  input: "Generated workflow"
  action: "Create metadata.json"
  output: "Workflow registration data"
```

### 6.2 Generated Workflow Structure

**Directory:** `.agent-os/workflows/test_generation_{language}_v1/`

```
test_generation_{language}_v1/
â”œâ”€â”€ metadata.json
â”œâ”€â”€ README.md
â”œâ”€â”€ phases/
â”‚   â”œâ”€â”€ 0/
â”‚   â”‚   â”œâ”€â”€ phase.md
â”‚   â”‚   â””â”€â”€ task-1-environment-setup.md
â”‚   â”œâ”€â”€ 1/
â”‚   â”‚   â”œâ”€â”€ phase.md
â”‚   â”‚   â”œâ”€â”€ task-1-function-analysis.md
â”‚   â”‚   â””â”€â”€ task-2-attribute-detection.md
â”‚   â”œâ”€â”€ 2/
â”‚   â”‚   â”œâ”€â”€ phase.md
â”‚   â”‚   â””â”€â”€ task-1-output-analysis.md
â”‚   â”œâ”€â”€ 3/
â”‚   â”‚   â”œâ”€â”€ phase.md
â”‚   â”‚   â””â”€â”€ task-1-dependency-analysis.md
â”‚   â”œâ”€â”€ 4/
â”‚   â”‚   â”œâ”€â”€ phase.md
â”‚   â”‚   â””â”€â”€ task-1-usage-patterns.md
â”‚   â”œâ”€â”€ 5/
â”‚   â”‚   â”œâ”€â”€ phase.md
â”‚   â”‚   â””â”€â”€ task-1-coverage-planning.md
â”‚   â”œâ”€â”€ 6/
â”‚   â”‚   â”œâ”€â”€ phase.md
â”‚   â”‚   â””â”€â”€ task-1-validation.md
â”‚   â”œâ”€â”€ 7/
â”‚   â”‚   â”œâ”€â”€ phase.md
â”‚   â”‚   â””â”€â”€ task-1-generation.md
â”‚   â””â”€â”€ 8/
â”‚       â”œâ”€â”€ phase.md
â”‚       â””â”€â”€ task-1-quality-gates.md
â””â”€â”€ templates/
    â”œâ”€â”€ unit-test-template.{ext}
    â”œâ”€â”€ integration-test-template.{ext}
    â””â”€â”€ validation-script.{ext}
```

### 6.3 Workflow Generation Command

**Usage:**

```bash
# Generate test generation workflow for detected language
python scripts/generate-test-workflow.py --language {language}

# Or auto-detect
python scripts/generate-test-workflow.py --auto-detect

# Output
Generated: .agent-os/workflows/test_generation_{language}_v1/
Registered: Workflow available via MCP
```

---

## 7. Implementation Roadmap

### Phase 1: Universal Standards Creation

**Deliverables:**
1. `universal/standards/testing/test-generation/methodology.md`
2. `universal/standards/testing/test-generation/path-system.md`
3. `universal/standards/testing/test-generation/quality-standards.md`
4. `universal/standards/testing/test-generation/evidence-tracking.md`
5. Phase-specific standards (phases 0-8)

**Timeline:** 1-2 weeks

### Phase 2: Language Instruction Templates

**Deliverables:**
1. Complete instruction template
2. Example: `language-instructions/python-test-generation.md` (comprehensive)
3. Example: `language-instructions/javascript-test-generation.md` (comprehensive)

**Timeline:** 1 week

### Phase 3: Additional Language Instructions

**Deliverables:**
1. `go-test-generation.md`
2. `typescript-test-generation.md`
3. `java-test-generation.md`
4. `rust-test-generation.md`
5. `csharp-test-generation.md`

**Timeline:** 2-3 weeks (1-2 days per language)

### Phase 4: Workflow Generation Automation

**Deliverables:**
1. `scripts/generate-test-workflow.py`
2. Language detection logic
3. Workflow template generation
4. Metadata generation
5. MCP integration

**Timeline:** 1-2 weeks

### Phase 5: Validation Scripts per Language

**Deliverables:**
1. Python validation script
2. JavaScript validation script
3. Go validation script
4. Java validation script
5. Rust validation script
6. TypeScript validation script
7. C# validation script

**Timeline:** 2 weeks

### Phase 6: Testing and Refinement

**Deliverables:**
1. Test each language workflow on sample projects
2. Measure success rates
3. Refine based on results
4. Document lessons learned

**Timeline:** 2-3 weeks

### Total Timeline: 9-13 weeks

---

## 8. Example Instantiations

### 8.1 Python Test Generation

**Universal Standard Application:**

```yaml
Phase_1_Function_Analysis:
  universal_requirement: "Extract all function signatures"
  python_implementation: "Use ast.parse() and ast.walk()"
  
  universal_requirement: "Document parameters and types"
  python_implementation: "Extract from ast.FunctionDef.args"
  
  universal_requirement: "Detect attribute access"
  python_implementation: "Find ast.Attribute nodes"
```

**Generated Task File:** `phases/1/task-1-function-analysis.md`

```markdown
# Task 1: Function Analysis (Python)

## Universal Requirements

> From: `universal/standards/testing/test-generation/phase-1-function-analysis.md`

1. Extract all function signatures
2. Document parameters and types
3. Identify return types
4. Detect attribute access patterns

## Python Implementation

### Step 1: AST-Based Function Extraction

ðŸ›‘ EXECUTE-NOW:

```python
python -c "
import ast
import sys

def analyze_functions(file_path):
    with open(file_path, 'r') as f:
        tree = ast.parse(f.read())
    
    functions = []
    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            args = [arg.arg for arg in node.args.args]
            defaults = len(node.args.defaults)
            required = len(args) - defaults
            
            # Extract return type if annotated
            return_type = (
                ast.unparse(node.returns) 
                if node.returns else 'Unknown'
            )
            
            functions.append({
                'name': node.name,
                'args': args,
                'required_args': required,
                'total_args': len(args),
                'return_type': return_type,
                'line': node.lineno
            })
    
    for func in functions:
        print(f\"{func['name']}({', '.join(func['args'])}) -> {func['return_type']} [Line {func['line']}]\")
    
    return len(functions)

count = analyze_functions(sys.argv[1])
print(f\"\\nðŸ“Š Total functions: {count}\")
" [PRODUCTION_FILE]
```

ðŸ›‘ PASTE-OUTPUT: Paste complete function list

ðŸ“Š COUNT-AND-DOCUMENT: Update progress table with function count

### Step 2: Attribute Access Detection

ðŸ›‘ EXECUTE-NOW:

```python
python -c "
import ast
import sys

def find_attributes(file_path):
    with open(file_path, 'r') as f:
        tree = ast.parse(f.read())
    
    attributes = set()
    for node in ast.walk(tree):
        if isinstance(node, ast.Attribute):
            # Get the attribute name
            attributes.add(node.attr)
    
    for attr in sorted(attributes):
        print(f\"  - {attr}\")
    
    return len(attributes)

print(\"Detected attributes:\")
count = find_attributes(sys.argv[1])
print(f\"\\nðŸ“Š Total unique attributes: {count}\")
" [PRODUCTION_FILE]
```

ðŸ›‘ PASTE-OUTPUT: Paste attribute list

### Step 3: Mock Completeness Requirements (Unit Path Only)

âš ï¸ IF UNIT PATH SELECTED:

Based on detected attributes, create mock completeness checklist:

```python
class MockObject:
    def __init__(self):
        self.attribute_1 = Mock()  # From analysis
        self.attribute_2 = Mock()  # From analysis
        # ... ALL detected attributes
```

ðŸ›‘ UPDATE-TABLE:

| Phase | Status | Evidence | Commands | Validation | Gate |
|-------|--------|----------|----------|------------|------|
| 1: Function Analysis | âœ… | X functions, Y attributes | 2/2 | Manual | âœ… |

## See Also

- [Universal Phase 1](../../../../universal/standards/testing/test-generation/phase-1-function-analysis.md)
- [Python AST Documentation](https://docs.python.org/3/library/ast.html)
```

### 8.2 JavaScript Test Generation

**Universal Standard Application:**

```yaml
Phase_1_Function_Analysis:
  universal_requirement: "Extract all function signatures"
  javascript_implementation: "Use @babel/parser or espree"
  
  universal_requirement: "Document parameters"
  javascript_implementation: "Extract from FunctionDeclaration.params"
  
  universal_requirement: "Detect attribute access"
  javascript_implementation: "Find MemberExpression nodes"
```

**Generated Task File:** `phases/1/task-1-function-analysis.md`

```markdown
# Task 1: Function Analysis (JavaScript)

## Universal Requirements

> From: `universal/standards/testing/test-generation/phase-1-function-analysis.md`

1. Extract all function signatures
2. Document parameters
3. Identify return types (if TypeScript)
4. Detect property access patterns

## JavaScript Implementation

### Step 1: Function Extraction

ðŸ›‘ EXECUTE-NOW:

```bash
# Install parser if needed
npm install --save-dev @babel/parser

# Extract functions
node -e "
const fs = require('fs');
const parser = require('@babel/parser');

const code = fs.readFileSync(process.argv[1], 'utf-8');
const ast = parser.parse(code, {
  sourceType: 'module',
  plugins: ['jsx', 'typescript']
});

function extractFunctions(node, functions = []) {
  if (node.type === 'FunctionDeclaration' || 
      node.type === 'FunctionExpression' ||
      node.type === 'ArrowFunctionExpression') {
    const params = node.params.map(p => p.name || 'destructured');
    const name = node.id ? node.id.name : '<anonymous>';
    functions.push({
      name: name,
      params: params,
      line: node.loc.start.line
    });
  }
  
  for (const key in node) {
    if (node[key] && typeof node[key] === 'object') {
      if (Array.isArray(node[key])) {
        node[key].forEach(child => extractFunctions(child, functions));
      } else {
        extractFunctions(node[key], functions);
      }
    }
  }
  
  return functions;
}

const functions = extractFunctions(ast);
functions.forEach(f => {
  console.log(\`\${f.name}(\${f.params.join(', ')}) [Line \${f.line}]\`);
});

console.log(\`\\nðŸ“Š Total functions: \${functions.length}\`);
" [TARGET_FILE]
```

ðŸ›‘ PASTE-OUTPUT: Paste complete function list

### Step 2: Property Access Detection

ðŸ›‘ EXECUTE-NOW:

```bash
grep -o "[a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*" [TARGET_FILE] | sort -u
```

ðŸ›‘ PASTE-OUTPUT: Paste property access list

### Step 3: Mock Completeness (Unit Path Only)

âš ï¸ IF UNIT PATH SELECTED:

Using Jest mocking:

```javascript
const mockObject = {
  property1: jest.fn(),
  property2: jest.fn(),
  // ... ALL detected properties
};
```

ðŸ›‘ UPDATE-TABLE:

| Phase | Status | Evidence | Commands | Validation | Gate |
|-------|--------|----------|----------|------------|------|
| 1: Function Analysis | âœ… | X functions, Y properties | 2/2 | Manual | âœ… |

## See Also

- [Universal Phase 1](../../../../universal/standards/testing/test-generation/phase-1-function-analysis.md)
- [Jest Mocking Documentation](https://jestjs.io/docs/mock-functions)
```

### 8.3 Go Test Generation

**Universal Standard Application:**

```yaml
Phase_1_Function_Analysis:
  universal_requirement: "Extract all function signatures"
  go_implementation: "Use go/parser and go/ast"
  
  universal_requirement: "Document parameters and types"
  go_implementation: "Extract from FuncDecl.Type.Params"
  
  universal_requirement: "Detect method calls"
  go_implementation: "Find SelectorExpr nodes"
```

**Generated Task File:** `phases/1/task-1-function-analysis.md`

```markdown
# Task 1: Function Analysis (Go)

## Universal Requirements

> From: `universal/standards/testing/test-generation/phase-1-function-analysis.md`

1. Extract all function signatures
2. Document parameters and types
3. Identify return types
4. Detect method calls

## Go Implementation

### Step 1: AST-Based Function Extraction

ðŸ›‘ EXECUTE-NOW:

Create temporary analysis script:

```go
// analyze_functions.go
package main

import (
	"fmt"
	"go/ast"
	"go/parser"
	"go/token"
	"os"
)

func main() {
	if len(os.Args) < 2 {
		fmt.Println("Usage: go run analyze_functions.go <file.go>")
		return
	}

	fset := token.NewFileSet()
	node, err := parser.ParseFile(fset, os.Args[1], nil, parser.ParseComments)
	if err != nil {
		panic(err)
	}

	var functions []string
	ast.Inspect(node, func(n ast.Node) bool {
		if fn, ok := n.(*ast.FuncDecl); ok {
			params := []string{}
			if fn.Type.Params != nil {
				for _, param := range fn.Type.Params.List {
					for _, name := range param.Names {
						params = append(params, name.Name)
					}
				}
			}
			
			returns := "unknown"
			if fn.Type.Results != nil && len(fn.Type.Results.List) > 0 {
				returns = fmt.Sprintf("%d return values", len(fn.Type.Results.List))
			}
			
			functions = append(functions, fmt.Sprintf("%s(%s) -> %s [Line %d]",
				fn.Name.Name,
				strings.Join(params, ", "),
				returns,
				fset.Position(fn.Pos()).Line,
			))
		}
		return true
	})

	for _, fn := range functions {
		fmt.Println(fn)
	}
	fmt.Printf("\nðŸ“Š Total functions: %d\n", len(functions))
}
```

```bash
go run analyze_functions.go [TARGET_FILE]
```

ðŸ›‘ PASTE-OUTPUT: Paste complete function list

### Step 2: Method Call Detection

ðŸ›‘ EXECUTE-NOW:

```bash
grep -oE "[a-zA-Z_][a-zA-Z0-9_]*\.[A-Z][a-zA-Z0-9_]*\(" [TARGET_FILE] | sort -u
```

ðŸ›‘ PASTE-OUTPUT: Paste method call list

### Step 3: Mock Strategy (Unit Path Only)

âš ï¸ IF UNIT PATH SELECTED:

Using testify/mock:

```go
type Mock{ObjectName} struct {
	mock.Mock
}

func (m *Mock{ObjectName}) Method1(args) returnType {
	args := m.Called(args)
	return args.Get(0).(returnType)
}
```

ðŸ›‘ UPDATE-TABLE:

| Phase | Status | Evidence | Commands | Validation | Gate |
|-------|--------|----------|----------|------------|------|
| 1: Function Analysis | âœ… | X functions, Y methods | 2/2 | Manual | âœ… |

## See Also

- [Universal Phase 1](../../../../universal/standards/testing/test-generation/phase-1-function-analysis.md)
- [Go testing package](https://pkg.go.dev/testing)
- [testify/mock](https://github.com/stretchr/testify)
```

---

## Conclusion

This design enables:

1. **Single Source of Truth:** Universal standards define methodology once
2. **Language Flexibility:** Language instructions adapt to any language
3. **Automated Generation:** LLM combines both to create workflows
4. **Proven Methodology:** 80%+ success rate pattern from V3 Python
5. **Maintainability:** Update universal standards â†’ all languages benefit

### Next Steps

1. Create universal standards (Layer 1)
2. Write language instructions (Layer 2)
3. Build workflow generator (automation)
4. Validate across languages
5. Iterate based on results

---

**Document End**

